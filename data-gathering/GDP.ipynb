{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/heliaa/Desktop/Tiva/data-gathering/GDP.ipynb Cell 1\u001b[0m in \u001b[0;36m1\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/heliaa/Desktop/Tiva/data-gathering/GDP.ipynb#W0sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mos\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/heliaa/Desktop/Tiva/data-gathering/GDP.ipynb#W0sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39m# Install Chrome driver\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/heliaa/Desktop/Tiva/data-gathering/GDP.ipynb#W0sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m ChromeDriverManager()\u001b[39m.\u001b[39minstall()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/heliaa/Desktop/Tiva/data-gathering/GDP.ipynb#W0sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m ser \u001b[39m=\u001b[39m Service()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/heliaa/Desktop/Tiva/data-gathering/GDP.ipynb#W0sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m driver \u001b[39m=\u001b[39m webdriver\u001b[39m.\u001b[39mChrome(service\u001b[39m=\u001b[39mser)\n",
      "File \u001b[0;32m~/tfod/lib/python3.9/site-packages/webdriver_manager/chrome.py:39\u001b[0m, in \u001b[0;36mChromeDriverManager.install\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39minstall\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mstr\u001b[39m:\n\u001b[0;32m---> 39\u001b[0m     driver_path \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_driver_binary_path(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdriver)\n\u001b[1;32m     40\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mall\u001b[39m(test_os \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m driver_path \u001b[39mfor\u001b[39;00m test_os \u001b[39min\u001b[39;00m [\u001b[39m\"\u001b[39m\u001b[39mmac_arm64\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mmac_x64\u001b[39m\u001b[39m\"\u001b[39m]):\n\u001b[1;32m     41\u001b[0m         os\u001b[39m.\u001b[39mchmod(driver_path, \u001b[39m0o755\u001b[39m)\n",
      "File \u001b[0;32m~/tfod/lib/python3.9/site-packages/webdriver_manager/core/manager.py:33\u001b[0m, in \u001b[0;36mDriverManager._get_driver_binary_path\u001b[0;34m(self, driver)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[39mif\u001b[39;00m binary_path:\n\u001b[1;32m     31\u001b[0m     \u001b[39mreturn\u001b[39;00m binary_path\n\u001b[0;32m---> 33\u001b[0m file \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_download_manager\u001b[39m.\u001b[39;49mdownload_file(driver\u001b[39m.\u001b[39;49mget_driver_download_url())\n\u001b[1;32m     34\u001b[0m binary_path \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_cache_manager\u001b[39m.\u001b[39msave_file_to_cache(driver, file)\n\u001b[1;32m     35\u001b[0m \u001b[39mreturn\u001b[39;00m binary_path\n",
      "File \u001b[0;32m~/tfod/lib/python3.9/site-packages/webdriver_manager/core/download_manager.py:32\u001b[0m, in \u001b[0;36mWDMDownloadManager.download_file\u001b[0;34m(self, url)\u001b[0m\n\u001b[1;32m     30\u001b[0m log(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mDriver downloading response is \u001b[39m\u001b[39m{\u001b[39;00mresponse\u001b[39m.\u001b[39mstatus_code\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     31\u001b[0m file_name \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mextract_filename_from_url(url)\n\u001b[0;32m---> 32\u001b[0m \u001b[39mreturn\u001b[39;00m File(response, file_name)\n",
      "File \u001b[0;32m~/tfod/lib/python3.9/site-packages/webdriver_manager/core/file_manager.py:12\u001b[0m, in \u001b[0;36mFile.__init__\u001b[0;34m(self, stream, file_name)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, stream, file_name):\n\u001b[0;32m---> 12\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcontent \u001b[39m=\u001b[39m stream\u001b[39m.\u001b[39;49mcontent\n\u001b[1;32m     13\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__stream \u001b[39m=\u001b[39m stream\n\u001b[1;32m     14\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfile_name \u001b[39m=\u001b[39m file_name\n",
      "File \u001b[0;32m~/tfod/lib/python3.9/site-packages/requests/models.py:899\u001b[0m, in \u001b[0;36mResponse.content\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    897\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_content \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    898\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 899\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_content \u001b[39m=\u001b[39m \u001b[39mb\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m.\u001b[39;49mjoin(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49miter_content(CONTENT_CHUNK_SIZE)) \u001b[39mor\u001b[39;00m \u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    901\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_content_consumed \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    902\u001b[0m \u001b[39m# don't need to release the connection; that's been handled by urllib3\u001b[39;00m\n\u001b[1;32m    903\u001b[0m \u001b[39m# since we exhausted the data.\u001b[39;00m\n",
      "File \u001b[0;32m~/tfod/lib/python3.9/site-packages/requests/models.py:816\u001b[0m, in \u001b[0;36mResponse.iter_content.<locals>.generate\u001b[0;34m()\u001b[0m\n\u001b[1;32m    814\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mraw, \u001b[39m\"\u001b[39m\u001b[39mstream\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m    815\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 816\u001b[0m         \u001b[39myield from\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mraw\u001b[39m.\u001b[39mstream(chunk_size, decode_content\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m    817\u001b[0m     \u001b[39mexcept\u001b[39;00m ProtocolError \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    818\u001b[0m         \u001b[39mraise\u001b[39;00m ChunkedEncodingError(e)\n",
      "File \u001b[0;32m~/tfod/lib/python3.9/site-packages/urllib3/response.py:627\u001b[0m, in \u001b[0;36mHTTPResponse.stream\u001b[0;34m(self, amt, decode_content)\u001b[0m\n\u001b[1;32m    625\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    626\u001b[0m     \u001b[39mwhile\u001b[39;00m \u001b[39mnot\u001b[39;00m is_fp_closed(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fp):\n\u001b[0;32m--> 627\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mread(amt\u001b[39m=\u001b[39;49mamt, decode_content\u001b[39m=\u001b[39;49mdecode_content)\n\u001b[1;32m    629\u001b[0m         \u001b[39mif\u001b[39;00m data:\n\u001b[1;32m    630\u001b[0m             \u001b[39myield\u001b[39;00m data\n",
      "File \u001b[0;32m~/tfod/lib/python3.9/site-packages/urllib3/response.py:566\u001b[0m, in \u001b[0;36mHTTPResponse.read\u001b[0;34m(self, amt, decode_content, cache_content)\u001b[0m\n\u001b[1;32m    563\u001b[0m fp_closed \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fp, \u001b[39m\"\u001b[39m\u001b[39mclosed\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m    565\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_error_catcher():\n\u001b[0;32m--> 566\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fp_read(amt) \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m fp_closed \u001b[39melse\u001b[39;00m \u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    567\u001b[0m     \u001b[39mif\u001b[39;00m amt \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    568\u001b[0m         flush_decoder \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/tfod/lib/python3.9/site-packages/urllib3/response.py:532\u001b[0m, in \u001b[0;36mHTTPResponse._fp_read\u001b[0;34m(self, amt)\u001b[0m\n\u001b[1;32m    529\u001b[0m     \u001b[39mreturn\u001b[39;00m buffer\u001b[39m.\u001b[39mgetvalue()\n\u001b[1;32m    530\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    531\u001b[0m     \u001b[39m# StringIO doesn't like amt=None\u001b[39;00m\n\u001b[0;32m--> 532\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fp\u001b[39m.\u001b[39;49mread(amt) \u001b[39mif\u001b[39;00m amt \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fp\u001b[39m.\u001b[39mread()\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/http/client.py:463\u001b[0m, in \u001b[0;36mHTTPResponse.read\u001b[0;34m(self, amt)\u001b[0m\n\u001b[1;32m    460\u001b[0m \u001b[39mif\u001b[39;00m amt \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    461\u001b[0m     \u001b[39m# Amount is given, implement using readinto\u001b[39;00m\n\u001b[1;32m    462\u001b[0m     b \u001b[39m=\u001b[39m \u001b[39mbytearray\u001b[39m(amt)\n\u001b[0;32m--> 463\u001b[0m     n \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mreadinto(b)\n\u001b[1;32m    464\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mmemoryview\u001b[39m(b)[:n]\u001b[39m.\u001b[39mtobytes()\n\u001b[1;32m    465\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    466\u001b[0m     \u001b[39m# Amount is not given (unbounded read) so we must check self.length\u001b[39;00m\n\u001b[1;32m    467\u001b[0m     \u001b[39m# and self.chunked\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/http/client.py:507\u001b[0m, in \u001b[0;36mHTTPResponse.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    502\u001b[0m         b \u001b[39m=\u001b[39m \u001b[39mmemoryview\u001b[39m(b)[\u001b[39m0\u001b[39m:\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlength]\n\u001b[1;32m    504\u001b[0m \u001b[39m# we do not use _safe_read() here because this may be a .will_close\u001b[39;00m\n\u001b[1;32m    505\u001b[0m \u001b[39m# connection, and the user is reading more bytes than will be provided\u001b[39;00m\n\u001b[1;32m    506\u001b[0m \u001b[39m# (for example, reading in 1k chunks)\u001b[39;00m\n\u001b[0;32m--> 507\u001b[0m n \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfp\u001b[39m.\u001b[39;49mreadinto(b)\n\u001b[1;32m    508\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m n \u001b[39mand\u001b[39;00m b:\n\u001b[1;32m    509\u001b[0m     \u001b[39m# Ideally, we would raise IncompleteRead if the content-length\u001b[39;00m\n\u001b[1;32m    510\u001b[0m     \u001b[39m# wasn't satisfied, but it might break compatibility.\u001b[39;00m\n\u001b[1;32m    511\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_close_conn()\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/socket.py:704\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    702\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m    703\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 704\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sock\u001b[39m.\u001b[39;49mrecv_into(b)\n\u001b[1;32m    705\u001b[0m     \u001b[39mexcept\u001b[39;00m timeout:\n\u001b[1;32m    706\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_timeout_occurred \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/ssl.py:1241\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1237\u001b[0m     \u001b[39mif\u001b[39;00m flags \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m   1238\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m   1239\u001b[0m           \u001b[39m\"\u001b[39m\u001b[39mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m\n\u001b[1;32m   1240\u001b[0m           \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m)\n\u001b[0;32m-> 1241\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mread(nbytes, buffer)\n\u001b[1;32m   1242\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1243\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39mrecv_into(buffer, nbytes, flags)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/ssl.py:1099\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1097\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1098\u001b[0m     \u001b[39mif\u001b[39;00m buffer \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m-> 1099\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sslobj\u001b[39m.\u001b[39;49mread(\u001b[39mlen\u001b[39;49m, buffer)\n\u001b[1;32m   1100\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1101\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sslobj\u001b[39m.\u001b[39mread(\u001b[39mlen\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "import time\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Install Chrome driver\n",
    "ChromeDriverManager().install()\n",
    "ser = Service()\n",
    "driver = webdriver.Chrome(service=ser)\n",
    "\n",
    "# Open the website\n",
    "driver.get(\"https://www.macrotrends.net/countries/USA/united-states/gdp-gross-domestic-product\")\n",
    "driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "time.sleep(10)\n",
    "\n",
    "# Find the table containing the data\n",
    "table = driver.find_element(By.XPATH, '//*[@id=\"main_content\"]/div[11]')\n",
    "\n",
    "# Extract the rows from the table\n",
    "rows = table.find_elements(By.TAG_NAME, \"tr\")\n",
    "\n",
    "# Initialize lists to store data\n",
    "data = []\n",
    "header = [\"Year\", \"GDP\"]\n",
    "\n",
    "# Loop through the rows and extract the data\n",
    "for row in rows[2:]:  # Start from the third row to skip the header rows\n",
    "    cells = row.find_elements(By.TAG_NAME, \"td\")\n",
    "    year = cells[0].text\n",
    "    gdp = cells[1].text.replace(\"$\", \"\").replace(\",\", \"\").replace(\"B\", \"\")  # Convert to numeric format\n",
    "    data.append([year, gdp])\n",
    "\n",
    "# Create a DataFrame from the data for Canada\n",
    "df = pd.DataFrame(data, columns=header)\n",
    "\n",
    "# Save the DataFrame to a CSV file for Canada\n",
    "df.to_csv(\"united-states_gdp_data.csv\", index=False)\n",
    "\n",
    "# Find the table containing the data\n",
    "table = driver.find_element(By.XPATH, '//*[@id=\"main_content\"]/div[9]')\n",
    "# Get the links for other countries\n",
    "country_links = table.find_elements(By.TAG_NAME, \"a\")\n",
    "country_list_href = [a.get_attribute('href') for a in country_links]\n",
    "\n",
    "# Loop through the links and save data for each country\n",
    "for link in country_list_href:\n",
    "        \n",
    "    driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "    driver.get(link) # Click on the link to navigate to the country's page\n",
    "    # driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "    time.sleep(5)  # Wait for the page to load\n",
    "    # Find the table containing the data for the country\n",
    "    country_table = driver.find_element(By.XPATH, '//*[@id=\"main_content\"]/div[11]')\n",
    "    country_rows = country_table.find_elements(By.TAG_NAME, \"tr\")\n",
    "    country_data = []\n",
    "    # Loop through the rows and extract the data for the country\n",
    "    for row in country_rows[2:]:  # Start from the third row to skip the header rows\n",
    "        cells = row.find_elements(By.TAG_NAME, \"td\")\n",
    "        year = cells[0].text\n",
    "        gdp = cells[1].text.replace(\"$\", \"\").replace(\",\", \"\").replace(\"B\", \"\")  # Convert to numeric format\n",
    "        country_data.append([year, gdp])\n",
    "    # Create a DataFrame from the data for the country\n",
    "    country_df = pd.DataFrame(country_data, columns=header)\n",
    "    # Extract the country name from the URL and use it as the CSV file name\n",
    "    country_name = link.split(\"/\")[-2]\n",
    "    # Save the DataFrame to a new CSV file for the current country\n",
    "    country_df.to_csv(f\"{country_name}_gdp_data.csv\", index=False)\n",
    "\n",
    "\n",
    "# Close the browser\n",
    "driver.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "import time\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Install Chrome driver\n",
    "ChromeDriverManager().install()\n",
    "ser = Service()\n",
    "driver = webdriver.Chrome(service=ser)\n",
    "\n",
    "# Open the website\n",
    "driver.get(\"https://www.macrotrends.net/countries/CHN/china/gdp-gross-domestic-product\")\n",
    "driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "time.sleep(10)\n",
    "\n",
    "# Find the table containing the data\n",
    "table = driver.find_element(By.XPATH, '//*[@id=\"main_content\"]/div[11]')\n",
    "\n",
    "# Extract the rows from the table\n",
    "rows = table.find_elements(By.TAG_NAME, \"tr\")\n",
    "\n",
    "# Initialize lists to store data\n",
    "data = []\n",
    "header = [\"Year\", \"GDP\"]\n",
    "\n",
    "# Loop through the rows and extract the data\n",
    "for row in rows[2:]:  # Start from the third row to skip the header rows\n",
    "    cells = row.find_elements(By.TAG_NAME, \"td\")\n",
    "    year = cells[0].text\n",
    "    gdp = cells[1].text.replace(\"$\", \"\").replace(\",\", \"\").replace(\"B\", \"\")  # Convert to numeric format\n",
    "    data.append([year, gdp])\n",
    "\n",
    "# Create a DataFrame from the data for Canada\n",
    "df = pd.DataFrame(data, columns=header)\n",
    "\n",
    "# Save the DataFrame to a CSV file for Canada\n",
    "df.to_csv(\"china_gdp_data.csv\", index=False)\n",
    "\n",
    "# Find the table containing the data\n",
    "table = driver.find_element(By.XPATH, '//*[@id=\"main_content\"]/div[9]')\n",
    "# Get the links for other countries\n",
    "country_links = table.find_elements(By.TAG_NAME, \"a\")\n",
    "country_list_href = [a.get_attribute('href') for a in country_links]\n",
    "\n",
    "# Loop through the links and save data for each country\n",
    "for link in country_list_href:\n",
    "        \n",
    "    driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "    driver.get(link) # Click on the link to navigate to the country's page\n",
    "    # driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "    time.sleep(5)  # Wait for the page to load\n",
    "    # Find the table containing the data for the country\n",
    "    country_table = driver.find_element(By.XPATH, '//*[@id=\"main_content\"]/div[11]')\n",
    "    country_rows = country_table.find_elements(By.TAG_NAME, \"tr\")\n",
    "    country_data = []\n",
    "    # Loop through the rows and extract the data for the country\n",
    "    for row in country_rows[2:]:  # Start from the third row to skip the header rows\n",
    "        cells = row.find_elements(By.TAG_NAME, \"td\")\n",
    "        year = cells[0].text\n",
    "        gdp = cells[1].text.replace(\"$\", \"\").replace(\",\", \"\").replace(\"B\", \"\")  # Convert to numeric format\n",
    "        country_data.append([year, gdp])\n",
    "    # Create a DataFrame from the data for the country\n",
    "    country_df = pd.DataFrame(country_data, columns=header)\n",
    "    # Extract the country name from the URL and use it as the CSV file name\n",
    "    country_name = link.split(\"/\")[-2]\n",
    "    # Save the DataFrame to a new CSV file for the current country\n",
    "    country_df.to_csv(f\"{country_name}_gdp_data.csv\", index=False)\n",
    "\n",
    "\n",
    "# Close the browser\n",
    "driver.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "import time\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Install Chrome driver\n",
    "ChromeDriverManager().install()\n",
    "ser = Service()\n",
    "driver = webdriver.Chrome(service=ser)\n",
    "\n",
    "# Open the website\n",
    "driver.get(\"https://www.macrotrends.net/countries/ETH/ethiopia/gdp-gross-domestic-product\")\n",
    "driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "time.sleep(10)\n",
    "\n",
    "# Find the table containing the data\n",
    "table = driver.find_element(By.XPATH, '//*[@id=\"main_content\"]/div[11]')\n",
    "\n",
    "# Extract the rows from the table\n",
    "rows = table.find_elements(By.TAG_NAME, \"tr\")\n",
    "\n",
    "# Initialize lists to store data\n",
    "data = []\n",
    "header = [\"Year\", \"GDP\"]\n",
    "\n",
    "# Loop through the rows and extract the data\n",
    "for row in rows[2:]:  # Start from the third row to skip the header rows\n",
    "    cells = row.find_elements(By.TAG_NAME, \"td\")\n",
    "    year = cells[0].text\n",
    "    gdp = cells[1].text.replace(\"$\", \"\").replace(\",\", \"\").replace(\"B\", \"\")  # Convert to numeric format\n",
    "    data.append([year, gdp])\n",
    "\n",
    "# Create a DataFrame from the data for Canada\n",
    "df = pd.DataFrame(data, columns=header)\n",
    "\n",
    "# Save the DataFrame to a CSV file for Canada\n",
    "df.to_csv(\"ethiopia_gdp_data.csv\", index=False)\n",
    "\n",
    "# Find the table containing the data\n",
    "table = driver.find_element(By.XPATH, '//*[@id=\"main_content\"]/div[9]')\n",
    "# Get the links for other countries\n",
    "country_links = table.find_elements(By.TAG_NAME, \"a\")\n",
    "country_list_href = [a.get_attribute('href') for a in country_links]\n",
    "\n",
    "# Loop through the links and save data for each country\n",
    "for link in country_list_href:\n",
    "        \n",
    "    driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "    driver.get(link) # Click on the link to navigate to the country's page\n",
    "    # driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "    time.sleep(5)  # Wait for the page to load\n",
    "    # Find the table containing the data for the country\n",
    "    country_table = driver.find_element(By.XPATH, '//*[@id=\"main_content\"]/div[11]')\n",
    "    country_rows = country_table.find_elements(By.TAG_NAME, \"tr\")\n",
    "    country_data = []\n",
    "    # Loop through the rows and extract the data for the country\n",
    "    for row in country_rows[2:]:  # Start from the third row to skip the header rows\n",
    "        cells = row.find_elements(By.TAG_NAME, \"td\")\n",
    "        year = cells[0].text\n",
    "        gdp = cells[1].text.replace(\"$\", \"\").replace(\",\", \"\").replace(\"B\", \"\")  # Convert to numeric format\n",
    "        country_data.append([year, gdp])\n",
    "    # Create a DataFrame from the data for the country\n",
    "    country_df = pd.DataFrame(country_data, columns=header)\n",
    "    # Extract the country name from the URL and use it as the CSV file name\n",
    "    country_name = link.split(\"/\")[-2]\n",
    "    # Save the DataFrame to a new CSV file for the current country\n",
    "    country_df.to_csv(f\"{country_name}_gdp_data.csv\", index=False)\n",
    "\n",
    "\n",
    "# Close the browser\n",
    "driver.quit()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
